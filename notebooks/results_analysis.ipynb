{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"../data/results\").resolve()\n",
    "fig_dir = Path(\"../figures\").resolve()\n",
    "DATA_FOLDER_PATH = Path(\"../data/processed/feature_extracted/pickle\").resolve()\n",
    "WIN_SIZES = [1024, 2048, 4096, 8192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the training and testing data from pickle files.\n",
    "\n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): Training data features.\n",
    "        y_train (np.ndarray): Training data target.\n",
    "        X_test (pd.DataFrame): Testing data features.\n",
    "    \"\"\"\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "\n",
    "    y_train = None\n",
    "\n",
    "    for win_size in WIN_SIZES:\n",
    "        train_df: pd.DataFrame = pd.read_pickle(\n",
    "            DATA_FOLDER_PATH / f\"train_features_{win_size}.pkl\"\n",
    "        )\n",
    "        X_test: pd.DataFrame = pd.read_pickle(\n",
    "            DATA_FOLDER_PATH / f\"test_features_{win_size}.pkl\"\n",
    "        )\n",
    "\n",
    "        X_train = train_df.drop(columns=[\"target\"], level=0)\n",
    "\n",
    "        train_dict[win_size] = X_train\n",
    "        test_dict[win_size] = X_test\n",
    "\n",
    "        if y_train is None:\n",
    "            y_train = train_df[\"target\"].values.ravel()\n",
    "\n",
    "    X_train = pd.concat(\n",
    "        train_dict.values(),\n",
    "        axis=1,\n",
    "        keys=WIN_SIZES,\n",
    "        names=[\"win_size\", \"feature\", \"stat\"],\n",
    "    )\n",
    "    X_test = pd.concat(\n",
    "        test_dict.values(),\n",
    "        axis=1,\n",
    "        keys=WIN_SIZES,\n",
    "        names=[\"win_size\", \"feature\", \"stat\"],\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, _ = load_data()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = pd.read_csv(\n",
    "    results_dir / \"logistic_regression_cv_results.csv\",\n",
    "    dtype={\"param_logreg__regularization\": str},\n",
    ")\n",
    "\n",
    "# Make columns a bit more readable\n",
    "lr_results.columns = lr_results.columns.str.replace(\"param_\\w+__\", \"\", regex=True)\n",
    "\n",
    "lr_results[\"regularization\"] = lr_results[\"regularization\"].fillna(\"None\")\n",
    "lr_results[\"lam\"] = lr_results[\"lam\"].fillna(\"NA\")\n",
    "lr_results[\"regularization\"] = lr_results[\"regularization\"].replace(\n",
    "    [\"l1\", \"l2\"], [\"L1\", \"L2\"]\n",
    ")\n",
    "\n",
    "# Filter out results which did not converge\n",
    "lr_results = lr_results.query(\"mean_test_score > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparisons Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logarithmic colormap\n",
    "cmap = mcolors.LogNorm(vmin=0.01, vmax=10)\n",
    "colors = plt.cm.viridis(cmap(np.linspace(0.01, 10, 4)))\n",
    "\n",
    "# Create a dictionary mapping hue levels to colors\n",
    "hue_dict = {\n",
    "    0.01: colors[0],\n",
    "    0.1: colors[1],\n",
    "    1.0: colors[2],\n",
    "    10: colors[3],\n",
    "    \"NA\": \"grey\",\n",
    "}\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    y=\"mean_test_score\",\n",
    "    data=lr_results,\n",
    "    x=\"regularization\",\n",
    "    hue=\"lam\",\n",
    "    palette=hue_dict,\n",
    ")\n",
    "# ax.set_title(\"Logistic Regression\\nCV Mean Test Accuracy by Regularization and Lambda\")\n",
    "ax.set_ylabel(\"CV Mean Test Accuracy\")\n",
    "ax.set_xlabel(\"Regularization Method\")\n",
    "_ = ax.legend(loc=\"lower left\", title=\"Lambda\")\n",
    "ax.set_ylim(0, 1)\n",
    "plt.savefig(\n",
    "    fig_dir / \"lr_accuracy_regularization_lam.png\", dpi=600, bbox_inches=\"tight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tolerance vs Fit Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tolerance vs Fit Time\n",
    "ax = sns.boxplot(y=\"mean_fit_time\", data=lr_results, x=\"tol\", hue=\"regularization\")\n",
    "ax.set_ylabel(\"CV Mean Fit Time (s)\")\n",
    "ax.set_xlabel(\"Loss Change Tolerance\")\n",
    "ax.legend(title=\"Regularization\")\n",
    "plt.savefig(fig_dir / \"lr_fit_time_tolerance.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Size vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Win size vs Accuracy\n",
    "ax = sns.boxplot(x=\"win_size\", y=\"mean_test_score\", data=lr_results)\n",
    "ax.set_ylabel(\"CV Mean Test Accuracy\")\n",
    "ax.set_xlabel(\"Window Size (N_FFT)\")\n",
    "ax.set_ylim(0, 1)\n",
    "plt.savefig(fig_dir / \"lr_accuracy_win_size_uniform.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CV Results to Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = lr_results.groupby([\"regularization\", \"lam\"])\n",
    "\n",
    "cols = [\"mean_test_score\", \"mean_fit_time\"]\n",
    "col_map = {\n",
    "    \"mean_test_score\": \"Mean Fold Test Accuracy\",\n",
    "    \"mean_fit_time\": \"Mean Fold Fit Time (s)\",\n",
    "}\n",
    "temp_dict = {}\n",
    "\n",
    "for col in cols:\n",
    "    temp_dict[col_map[col]] = gb[col].describe()[[\"mean\", \"std\"]]\n",
    "\n",
    "temp_df = pd.concat(temp_dict, axis=1)\n",
    "temp_df = temp_df.rename_axis([\"Regularization\", \"Lambda\"])\n",
    "\n",
    "# Set lambdas to scientific notation\n",
    "temp_df.index = temp_df.index.set_levels(\n",
    "    temp_df.index.levels[1].map(lambda x: f\"{float(x):.0e}\" if x != \"NA\" else x),\n",
    "    level=1,\n",
    ")\n",
    "temp_df.to_latex(\n",
    "    results_dir / \"lr_results_summary.tex\",\n",
    "    float_format=\"%.3f\",\n",
    "    caption=\"Logistic Regression 5-Fold CV Results Summary\",\n",
    "    label=\"tab:lr_results_summary\",\n",
    ")\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_window_sizes = [2048, \"All\"]\n",
    "n_components = 5\n",
    "fig, axs = plt.subplots(nrows=len(plt_window_sizes), ncols=1, figsize=(5, 5))\n",
    "\n",
    "for ax, win_size in zip(axs, plt_window_sizes):\n",
    "    if win_size == \"All\":\n",
    "        pca = PCA(n_components=n_components).fit(x_train)\n",
    "    else:\n",
    "        pca = PCA(n_components=n_components).fit(x_train[win_size])\n",
    "\n",
    "    sns.lineplot(\n",
    "        np.cumsum(pca.explained_variance_ratio_),\n",
    "        ax=ax,\n",
    "        color=\"red\",\n",
    "        label=\"Cumulative Explained Variance\",\n",
    "    )\n",
    "    ax = sns.barplot(pca.explained_variance_ratio_, ax=ax)\n",
    "\n",
    "    _ = ax.bar_label(ax.containers[0], fontsize=10, fmt=\"{:.2%}\")\n",
    "    ax.set(\n",
    "        title=f\"Window Size: {win_size}\",\n",
    "        xlabel=\"Principal Component\",\n",
    "        ylabel=\"Explained Variance\",\n",
    "    )\n",
    "    ax.legend(loc=\"center right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / \"pca_explained_variance.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=60, random_state=42)\n",
    "X_tsne = tsne.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y_train_one_hot.shape[1]\n",
    "perplexities = [5, 30, 50, 100, 150]\n",
    "\n",
    "\n",
    "def fix_label(l):\n",
    "    if l == \"1.0\":\n",
    "        return \"Genre\"\n",
    "    return \"Rest\"\n",
    "\n",
    "\n",
    "# Plot a t-SNE for each perplexity to see what looks best\n",
    "for p in perplexities:\n",
    "    tsne = TSNE(perplexity=p, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(x_train)\n",
    "    ncols = 2\n",
    "    fig, axs = plt.subplots(nrows=n_classes // ncols, ncols=ncols, figsize=(8, 16))\n",
    "\n",
    "    # Plot a t-SNE for each class since there's many classes\n",
    "    for i in range(n_classes):\n",
    "        genre = encoder.categories_[0][i]\n",
    "        ax = axs[i // ncols, i % ncols]\n",
    "        ax = sns.scatterplot(\n",
    "            x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_train_one_hot[:, i], ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"{genre.title()}\")\n",
    "        ax.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n",
    "        ax.legend_.remove()\n",
    "\n",
    "        # Steal the legend content from the first plot\n",
    "        if i == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.025)\n",
    "    fig.legend(\n",
    "        handles=handles, labels=map(fix_label, labels), loc=\"lower center\", ncol=2\n",
    "    )\n",
    "\n",
    "    plt.savefig(fig_dir / f\"tsne_perplexity_{p}.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs529_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
